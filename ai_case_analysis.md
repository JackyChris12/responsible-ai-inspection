**# AI in the Real World — Judge the Bot**



Prepared by: Jackline Kibiwot  

Role: Responsible AI Inspector







**## Case 1: Hiring Bot Bias**



\*\***What’s Happening:\*\***  

A company uses an AI system to screen job applicants. However, it tends to reject more female applicants who have gaps in their CVs, such as for maternity leave.



**What’s Problematic:**  

The AI is likely trained on biased historical data. If previous hiring favored uninterrupted careers (mostly men), the AI learns to repeat that pattern, causing gender discrimination.



**Responsible Improvement:**  

\- Retrain the AI using inclusive and diverse data.

\- Add transparency to explain why applicants are rejected.

\- Include human review for flagged applications.







&nbsp;**Case 2: School Proctoring AI**



**What’s Happening:** 

An AI system monitors students during online exams and flags them as cheating based on eye movement.



**What’s Problematic:** 

The system often wrongly flags neurodivergent students or those with anxiety. It assumes all students behave the same, which leads to unfair judgments.



**Responsible Improvement:**  

\- Adjust the AI to recognize a wider range of behaviors.

\- Use human judgment alongside AI alerts.

\- Allow students to appeal or view the flagged decisions.






# AI in the Real World â€” Judge the Bot

Prepared by: Jackline Kibiwot  
Role: Responsible AI Inspector

---

## Case 1: Hiring Bot Bias

**What is happening:**  
A company uses an AI system to screen job applicants. The system tends to reject more female applicants who have gaps in their CVs, such as for maternity leave.

**What is problematic:**  
The AI is trained on historical hiring data that reflects bias. It assumes that continuous work history equals better candidates, which disadvantages women and caregivers. This leads to unfair outcomes.

**One responsible improvement:**  
- Train the AI on more inclusive data that represents different career paths.
- Add transparency so applicants can understand why they were rejected.
- Have a human review decisions, especially when career gaps are involved.



## Case 2: School Proctoring AI

**What is happening:**  
An AI monitors students during online exams and flags them as cheating based on eye movement or facial behavior.

**What is problematic:**  
The system often flags neurodivergent students or those with anxiety, who may move or behave differently. It assumes everyone behaves the same during exams, which is unfair and causes stress.

**One responsible improvement:**  
- Make the AI more flexible to different behaviors.
- Use human reviewers to confirm before making accusations.
- Let students see and appeal flagged decisions.

